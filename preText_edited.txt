RWKV: Reinventing RNNs for the Transformer Era

Abstract
Transformers have revolutionized almost all natural language processing (NLP) tasks but suffer from memory and computational complexity that scales quadratically with sequence length. In contrast, recurrent neural networks (RNNs) exhibit linear scaling in memory and
computational requirements but struggle to match the same performance as Transformers due to limitations in parallelization and scalability.
We propose a novel model architecture, Receptance Weighted Key Value (RWKV), that combines the efﬁcient parallelizable training of Transformers with the efﬁcient inference of R NNs. Our approach leverages a linear attention mechanism and allows us to formulate the model as either a Transformer or an RNN, which parallelizes computations during training and maintains constant computational and memory complexity during inference, leading to the ﬁrst non-transformer architecture to be scaled to tens of billions of parameters.
Our experiments reveal that RWKV performs on par with similarly sized Transformers, suggesting that future work can leverage this architecture to create more efﬁcient models. 
This work presents a signiﬁcant step towards reconciling the trade-offs between computational efﬁciency and model performance in sequence processing tasks.

Introduction
Deep learning techniques have made signiﬁcant strides in artiﬁcial intelligence, playing a pivotal role in various scientiﬁc and industrial  applications. These applications often involve complex sequential data processing tasks that include natural language understanding, conversational AI, time-series analysis, and even indirect modalities that can be reframed as sequences, such as images and graphs (Brown et al., 2020; Ismail Fawaz et al., 2019; Wu et al., 2020; Albalak et al., 2022).
Predominant among these techniques are RNNs,convolutional neural networks (CNNs), and the Transformer models (Vaswani et al., 2017). 
Each of these has distinct drawbacks that restrict their efﬁciency in certain scenarios. RNNs suffer from the vanishing gradient problem, making them difﬁcult to train for long sequences. Additionally, they cannot be parallelized in the time dimension during training, which restricts their scalability (Hochreiter, 1998; Le and Zuidema, 2016). CNNs, on the other hand, are only adept at capturing local patterns, which limits their capacity to deal with long-range dependencies,  crucial to many sequence processing tasks (Bai et al., 2018).
Transformer models emerged as a powerful alternative due to their ability to handle both local and long-range dependencies and their capability for parallelized training (Tay et al., 2022). Recent models such as GPT-3 (Brown et al., 2020), ChatGPT (OpenAI, 2022; Koco´n et al., 2023), GPT-4 (OpenAI, 2023), LLaMA (Touvron et al., 2023), and Chinchilla (Hoffmann et al., 2022) exemplify the capability of this architecture, pushing the frontiers of what’s possible in NLP. Despite these signiﬁcant advancements, the self-attention mechanism
inherent to Transformers poses unique challenges, primarily due to its quadr atic complexity. This complexity renders the architecture computationally expensive and memory-intensive for tasks involving long input sequences or in resource-constrained situations. These limitations have spurred a wealth of research aiming to improve the scaling properties of Transformers, often at the expense of some of
the properties that make it so effective (Wang et al.,2020; Zaheer et al., 2020; Dao et al., 2022a).
To tackle these challenges, we introduce the Receptance Weighted Key Value (RWKV) model, a  novel architecture that effectively combines the strengths of RNNs and Transformers while circumventing key drawbacks. RWKV is carefully designed to alleviate the memory bottleneck and quadratic scaling associated with Transformers (Katharopoulos et al., 2020) with a more efﬁcient linear scaling, while still preserving the rich, expressive properties that make the Transformer a dominant architecture in the ﬁeld.
One of the deﬁning characteristics of RWKV is its ability to offer parallelized training and robust scalability, similar to Transformers. Moreover, we have reformulated the attention mechanism in RWKV to introduce a variant of linear attention, eschewing the traditional dot-product token interaction in favor of more effective channel-directed attention. This approach contrasts signiﬁcantly with the traditional Transformer architecture, where speciﬁc token interactions predominantly drive attention. 
The implementation of linear attention in RWKV is carried out without approximation, which offers a considerable improvement in efﬁciency and enhances the scalability, see Table 1.
The overarching motivation behind developing RWKV is to bridge the gap between computational efﬁciency and expressive capacity in neural network architectures. It offers a promising and viable solution for handling tasks involving large-scale models with billions of parameters, exhibiting competitive performance at a fraction of the computational cost. Our experimental results suggest that RWKV could be a valuable tool for addressing the ongoing challenges in scaling and deploying AI models across various domains, particularly those involving sequential data processing. Thus, RWKV paves the way for the next generation of more sustainable and computationally efﬁcient AI models for sequence processing tasks.
Our contributions in this paper are as follows:
• We introduce the RWKV network architecture, which combines the advantages of RNNs and Transformers while mitigating their known limitations.
• We propose a new attention mechanism reformulation that results in linear attention, eschewing the quadratic complexity associated with standard Transformer models.
• We conduct a comprehensive series of experiments on benchmark datasets to showcase the performance, efﬁciency and scaling of RWKV in managing tasks involving large-scale models and long-range dependencies.
• We release pretrained model ranging in size from 169 million to 14 billion parameters trained on the Pile (Gao et al., 2020)

Related Work
Recently, a number of techniques have been proposed to address the limitations of transformers. 
Optimizing Attention Mechanism
Many transformer variants (“ x-formers”) have been introduced to reduce the complexity of transformers (Tay et al.,2022), including sparse attention (Beltagy et al., 2020; Kitaev et al., 2020; Guo et al., 2022), approximating the full attention matrix (Wang et al., 2020; Ma et al., 2021; Choromanski et al., 2020), combining chunked attention with gating (Ma et al.,2023) and other efﬁcient methods (Katharopoulos
et al., 2020;  Jaegle et al., 2021). Some recent works like FlashAttention (Dao et al., 2022a) and others (Rabe and Staats, 2022;Jang et al., 2019) share similarities with RWKV’s chunked computation scheme. Despite being memory-efﬁcient, their time complexity remains
quadratic or contains chunk size as a hidden factor. In contrast, RWKV achieves better space and time complexity during inference by formulating a linear attention as an RNN.
Attention Free Models
Another line of research replaces the attention mechanism with other modules to scale to long sequences. MLP-Mixer and others (Tolstikhin et al., 2021; Liu et al., 2021) proposed the replacement of attention by Multi-Layer Perceptrons (MLPs) in computer vision tasks.
The Attention Free Transformer (AFT) (Zhai et al.,2021) replaces dot-product self-attention with a computationally  efﬁcient alternative which can be seen as a multi-head attention where each feature dimension corresponds to a head. Inspired by AFT, RWKV takes a similar approach but modiﬁes the interaction weights for simplicity such that it can be transformed into an RNN. In parallel, RNN-style (Hochreiter and Schmidhuber, 1997; Chung et al., 2014) recursive components have also been modiﬁed to increase context length, such as the Recurrent Memory Transformer (Bulatov et al ., 2022,2023) and Linear Recurrent Units (Orvieto et al.,2023). State space models (SSM) like S4 (Gu et al.,2022) and its variants (Dao et al., 2022b; Poli et al.,2023) are also proposed.
Notably, Quasi-Recurrent neural network (QRNN) (Bradbury et al., 2017) uses both convolutional layers and recurrent pooling functions
across timesteps and channels. While QRNN utilizes conv olutional ﬁlters with ﬁxed sizes, RWKV employs a time-mixing module as an
attention mechanism with time-decaying factors. Different from the element-wise pooling in QRNN, RWKV includes a parametrized channel-mixing module (see the green blocks in Fig.1c) that is parallelizable.

